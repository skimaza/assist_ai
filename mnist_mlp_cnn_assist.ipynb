{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTz+cn+cmBMSmCvvAvVsO2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skimaza/assist_ai/blob/main/mnist_mlp_cnn_assist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j3a7hs95ZEK"
      },
      "source": [
        "# 딥러닝의 이해\n",
        "# CNN을 이용한 MNIST 필기체 숫자 인식\n",
        "## MNIST dataset : http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlBGjhch57W6"
      },
      "source": [
        "# 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3J4J2Rvhd33"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE2BNl2D4q-W"
      },
      "source": [
        "학습 실행 시간을 표시하기 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3u6kh5x4pKz"
      },
      "source": [
        "import time\n",
        "\n",
        "def timer(start, end):\n",
        "    hours, rem = divmod(end-start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ETk-qJ88pvF"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wby95NM28sLG"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS9esMsv6ZNR"
      },
      "source": [
        "# PyTorch에서 제공하는 공개 데이터셋 라이브러리 사용\n",
        "transform parameter는 데이터셋에 자동으로 적용할 데이터 변환 기능. 여기서는 데이터값을 파이토치 Tensor로 바꾸는 기능만 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMMs-JSbCM_3"
      },
      "source": [
        "transform = transforms.ToTensor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIQ7ziAZ6O9G"
      },
      "source": [
        "# choose the training and testing datasets\n",
        "train_data = datasets.MNIST(root = 'data', train = True, download = True, transform = transform)\n",
        "test_data = datasets.MNIST(root = 'data', train = False, download = True, transform = transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0IyebvYg047"
      },
      "source": [
        "#dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbyGUDZfh8Y4"
      },
      "source": [
        "#dataset2 = datasets.MNIST('../data', train=False, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oAygvjt7vVa"
      },
      "source": [
        "# 다운로드된 데이터 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQerthlhfcyU"
      },
      "source": [
        "type(train_data), type(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM90UWcWe9ro"
      },
      "source": [
        "print(train_data)\n",
        "print(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niUCeG9F8hps"
      },
      "source": [
        "# 다운로드된 디렉토리 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPHG7I7dfQfV"
      },
      "source": [
        "!ls data/MNIST/raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN122sXCjahL"
      },
      "source": [
        "### (Tensor, label) format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LjYWMcT83T6"
      },
      "source": [
        "# 전체 데이터 수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_MJVCMnfmCF"
      },
      "source": [
        "len(train_data), type(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fSD4hY-9HHJ"
      },
      "source": [
        "# 첫번째 데이터의 형태"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJPFwcAggMj_"
      },
      "source": [
        "len(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCAw9qk59RAs"
      },
      "source": [
        "type(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LF6D_rX9WqU"
      },
      "source": [
        "len(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP6GFuW09NvB"
      },
      "source": [
        "type(train_data[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELBd6dPA9dTQ"
      },
      "source": [
        "train_data[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2bkKTgdgjos"
      },
      "source": [
        "train_data[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEn_KrCf9q8P"
      },
      "source": [
        "1개의 28x28 디멘션을 가지는 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOtcWRsJgWdx"
      },
      "source": [
        "train_data[0][1] # label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SP4NLYW-Jdr"
      },
      "source": [
        "train_data[0][0]에 대한 label. 즉 이미지가 표현하는 십진수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q9jbAAAh9tr"
      },
      "source": [
        "## 값의 범위 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I67Xdo_nhVzd"
      },
      "source": [
        "for i in range(5):\n",
        "    print('Max value:', train_data[i][0].max())\n",
        "    print('Min Value:', train_data[i][0].min())\n",
        "    print('Label', train_data[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Vn1vtz-jo6"
      },
      "source": [
        "데이터셋의 처음 5개 데이터는 0과 1사이의 픽셀값을 갖는 흑백이미지이고 레이블은 각각 5, 0, 4, 1, 9이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P47Ev444-4Fr"
      },
      "source": [
        "# 첫번째 데이터의 일부분만 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_owSHqWqhJpM"
      },
      "source": [
        "train_data[0][0][:15, :15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHMVuPuy_C7Y"
      },
      "source": [
        "train_data 값의 형태 재확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxy7CxRDirXt"
      },
      "source": [
        "type(train_data[0][0]), train_data[0][0].shape, type(train_data[0][1]), train_data[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ebawXZf2G7"
      },
      "source": [
        "### 이미지를 직접 보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X459mXn3_R-t"
      },
      "source": [
        "이미지를 디스플레이하기 위해 텐서를 numpy 배열로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r5aMELYirTP"
      },
      "source": [
        "img0 = train_data[0][0].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-eqPbq0_eYY"
      },
      "source": [
        "matplotlib 기능을 이용하여 디스플레이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue-dauYLkhOg"
      },
      "source": [
        "plt.imshow(np.squeeze(img0), 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMuSTSz88PM5"
      },
      "source": [
        "# # convert data to torch.FloatTensor\n",
        "# transform = transforms.ToTensor()\n",
        "# # choose the training and testing datasets\n",
        "# train_data = datasets.MNIST(root = 'data', train = True, download = True, transform = transform)\n",
        "# test_data = datasets.MNIST(root = 'data', train = False, download = True, transform = transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU41SyZqB9yz"
      },
      "source": [
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_index, valid_index = indices[split:], indices[:split]\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_index)\n",
        "valid_sampler = SubsetRandomSampler(valid_index)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, \n",
        "                                           sampler = train_sampler, num_workers = num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n",
        "                                          sampler = valid_sampler, num_workers = num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, num_workers = num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkViTOUhirRf"
      },
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    # print out the correct label for each image\n",
        "    # .item() gets the value contained in a Tensor\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbEVM7DZirGg"
      },
      "source": [
        "img = np.squeeze(images[1])\n",
        "fig = plt.figure(figsize = (12,12)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')\n",
        "width, height = img.shape\n",
        "thresh = img.max()/2.5\n",
        "for x in range(width):\n",
        "    for y in range(height):\n",
        "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "        ax.annotate(str(val), xy=(y,x),\n",
        "                    horizontalalignment='center',\n",
        "                    verticalalignment='center',\n",
        "                    color='white' if img[x][y]<thresh else 'black')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tzTs4uWMmDo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoxWK_38M7lW"
      },
      "source": [
        "# 베이스라인 MLP 모델 (지난 시간 예제)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aSkeR3QAyDU"
      },
      "source": [
        "# fully connected layer 2개, 마지막 fully connected output layer가 1개인 Multilayer Perceptron 네트워크 정의\n",
        "### PyTorch에서는 class method인 forward에서 forward propagation을 정의하면 backward propagation 부분은 자동으로 구성해 줌 (autograd 기능)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt0UJFoYiO0H"
      },
      "source": [
        "# define NN architecture\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP,self).__init__()\n",
        "        # number of hidden nodes in each layer (512)\n",
        "        hidden_1 = 512\n",
        "        hidden_2 = 512\n",
        "        # linear layer (784 -> hidden_1)\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        # linear layer (n_hidden -> hidden_2)\n",
        "        self.fc2 = nn.Linear(512,512)\n",
        "        # linear layer (n_hidden -> 10)\n",
        "        self.fc3 = nn.Linear(512,10)\n",
        "        # dropout layer (p=0.2)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        # flatten image input. 20개의 28*28 크기의 3차원 입력데이터([20,28,28])인 x를 28*28(=784) 크기의 2차원 데이터로 변환([20,784]).\n",
        "        # 여기서 정의한 MLP 네트워크에서는 2차원 정보를 이용하지 않음\n",
        "        x = x.view(-1,28*28)\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "         # add hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add output layer\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im4wVMcQBjHS"
      },
      "source": [
        "# initialize the NN\n",
        "model_mlp = MLP()\n",
        "print(model_mlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01VcOw8mDClv"
      },
      "source": [
        "model은 784 -> 512 -> 512 -> 10 의 구조를 가진 MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frgQ9-t-DR4Y"
      },
      "source": [
        "# Loss (Error)는 분류 문제에 사용하는 CrossEntropyLoss를 사용\n",
        "# Gradient descent 알고리즘은 SGD(Stochastic Gradient Descent)를 사용\n",
        "# Learning rate는 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7Ojp9FzMzgA"
      },
      "source": [
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
        "optimizer = torch.optim.SGD(model_mlp.parameters(),lr = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8HyZJKvFFv9"
      },
      "source": [
        "# 학습 실행\n",
        "### n_epochs를 50으로 설정 (학습에 시간이 걸리므로 지난 시간 결과를 참고해도 됨)\n",
        "### 학습 1번 완료 후 train_data의 loss와 valid_data의 loss와 비교\n",
        "### valid loss가 줄어들었으면 모델을 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJTuxxQAMzgB"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 50\n",
        "#n_epochs = 25\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor losses\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model_mlp.train() # prep model for training\n",
        "    for data,label in train_loader:\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_mlp(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output,label)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        \n",
        "     ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model_mlp.eval()  # prep model for evaluation\n",
        "    for data,label in valid_loader:\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_mlp(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output,label)\n",
        "        # update running validation loss \n",
        "        valid_loss += loss.item() * data.size(0) # bug\n",
        "    \n",
        "    # print training/validation statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss / len(train_loader.sampler)\n",
        "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
        "    \n",
        "    end = time.time()\n",
        "    print('Epoch: {} Elapsed time:{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        timer(start, end),\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "        ))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model_mlp.state_dict(), 'model_mlp.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JiPfuH4Itmj"
      },
      "source": [
        "# 저장된 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2MXxdYuMzgB"
      },
      "source": [
        "model_mlp.load_state_dict(torch.load('model_mlp.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg_Md95MIy1I"
      },
      "source": [
        "# test 데이터셋에 대해 인식률 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl21d3FIMzgC"
      },
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "model_mlp.eval() # prep model for evaluation\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model_mlp(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(len(target)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.sampler)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ug1_a6FI7-F"
      },
      "source": [
        "# 약 98% 정확도 달성\n",
        "# 실제 이미지를 보고 성능 확인\n",
        "아래 예제에서는 하나의 미니배치(20개) 이미지를 확인함  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVUgEKaNMzgD"
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "# get sample outputs\n",
        "output = model_mlp(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds = torch.max(output, 1)\n",
        "# prep images for display\n",
        "images = images.numpy()\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "max_images = batch_size\n",
        "for idx in np.arange(max_images):\n",
        "    ax = fig.add_subplot(2, max_images/2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azzStf5fsu8l"
      },
      "source": [
        "오류인 경우를 보기위해 틀릴 때까지 반복"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIn4-BMjsrcL"
      },
      "source": [
        "fail = False\n",
        "for images, labels in dataiter:\n",
        "    # get sample outputs\n",
        "    output = model_mlp(images)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, preds = torch.max(output, 1)\n",
        "    # prep images for display\n",
        "    images = images.numpy()\n",
        "    # plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(25, 4))\n",
        "    for idx in np.arange(20):\n",
        "        ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "        ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "        ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
        "        if preds[idx] != labels[idx]:\n",
        "            fail = True\n",
        "    plt.show()\n",
        "    if fail:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjZH-JqTozgj"
      },
      "source": [
        "**<font color=\"red\" size=\"4\">여기까지 지난 시간 예제와 동일</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6JtpW65XT5f"
      },
      "source": [
        "# 기본 구조 CNN (Vanailla CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MPk9FQndtct"
      },
      "source": [
        "* 두 Conv 레이어 정의\n",
        "    * 1 to 32 : 입력이 1 채널, 32개의 3x3 커널 학습, 이동(stride)은 한 칸씩\n",
        "    * 32 to 64 : 입력이 32 채널, 64개의 3x3 커널 학습, 이동(stride)은 한 칸씩\n",
        "\n",
        "* 레이어 별 dropout 정의\n",
        "    * 25%를 dropout, 50%를 dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj0CBDVCY8wv"
      },
      "source": [
        "### no dropout, padding 1, two CONV layer->Max pool->two FC layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk2_dVECsEWg"
      },
      "source": [
        "nn.Conv2d(in_channels, out_channels, kernel_size)  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klg4hF_QXSSz"
      },
      "source": [
        "class MnistVanillaCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistVanillaCNN, self).__init__() # 여기도 함께 바꿔줘야 함\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(7*7*64, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    # weight가 없는 activation function은 __init__에서 변수로 저장하지 않아도 된다\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x) # Conv -> ReLU\n",
        "        x = F.relu(x)     # nn.ReLU와 동일한 계산을 수행. F.relu()는 계산, nn.ReLU()는 계산을 수행하는 모듈을 리턴함.\n",
        "        x = F.max_pool2d(x, 2)  # max pooling 도 weight가 없음.\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1) # flatten before linear layer\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model_v = MnistVanillaCNN()\n",
        "print(model_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lkzwwr9Ziv_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAm0Q239Z3PS"
      },
      "source": [
        "## Vanilla CNN train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2DZcEkQiOwO"
      },
      "source": [
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
        "#optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)\n",
        "optimizer_v = torch.optim.SGD(model_v.parameters(),lr = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMeGs1uCZ1rZ"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 25\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "start = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor losses\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    \n",
        "     \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model_v.train() # prep model for training\n",
        "    for data,label in train_loader:\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer_v.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_v(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, label)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer_v.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        \n",
        "        \n",
        "     ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model_v.eval()  # prep model for evaluation\n",
        "    for data, label in valid_loader:\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_v(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, label)\n",
        "        # update running validation loss \n",
        "        valid_loss += loss.item() * data.size(0) # bug\n",
        "    \n",
        "    # print training/validation statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss / len(train_loader.sampler)\n",
        "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
        "    \n",
        "    end = time.time()\n",
        "    print('Vanilla: Epoch:{} Elapsed time:{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        timer(start, end),\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "        ))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model_v.state_dict(), 'model_v.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tXANQMOjAMm"
      },
      "source": [
        "저장된 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhHq-a7giUFh"
      },
      "source": [
        "model_v.load_state_dict(torch.load('model_v.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST4pUirHiXIq"
      },
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10)) # 숫자별 맞힌 개수\n",
        "class_total = list(0. for i in range(10)) # 숫자별 총 테스트 개수\n",
        "model_v.eval() # prep model for evaluation\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model_v(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(len(target)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.sampler)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJhsWMheiog2"
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1criGN_YCLJ4"
      },
      "source": [
        "처음 20개 샘플 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io_zU8GOAmt9"
      },
      "source": [
        "images, labels = dataiter.next()\n",
        "# get sample outputs\n",
        "output = model_v(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds = torch.max(output, 1)\n",
        "# prep images for display\n",
        "images = images.numpy()\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5EgqHOjA6c0"
      },
      "source": [
        "오류인 경우를 보기위해 틀릴 때까지 반복"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HW6qM7oA5Ix"
      },
      "source": [
        "fail = False\n",
        "for images, labels in dataiter:\n",
        "    # get sample outputs\n",
        "    output = model_v(images)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, preds = torch.max(output, 1)\n",
        "    # prep images for display\n",
        "    images = images.numpy()\n",
        "    # plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(25, 4))\n",
        "    for idx in np.arange(20):\n",
        "        ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "        ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "        ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
        "        if preds[idx] != labels[idx]:\n",
        "            fail = True\n",
        "    plt.show()\n",
        "    if fail:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahy8TsQ2x_q1"
      },
      "source": [
        "## dropout regularization 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP9ZYcfz5RJg"
      },
      "source": [
        "class MnistCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistCNN, self).__init__() # 여기도 함께 바꿔줘야 함\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(7*7*64, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    # weight가 없는 activation function은 __init__에서 변수로 저장하지 않아도 된다\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x) # Conv -> ReLU\n",
        "        x = F.relu(x)     # nn.ReLU와 동일한 계산을 수행. F.relu()는 계산, nn.ReLU()는 계산을 수행하는 모듈을 리턴함.\n",
        "        x = F.max_pool2d(x, 2) # max pooling 도 weight가 없음.\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "        \n",
        "# initialize the NN\n",
        "model = MnistCNN()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLjlERieZ54a"
      },
      "source": [
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXzAHKsqiOuV"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 25\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "start = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor losses\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    \n",
        "     \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train() # prep model for training\n",
        "    for data,label in train_loader:\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output,label)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        \n",
        "        \n",
        "     ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()  # prep model for evaluation\n",
        "    for data,label in valid_loader:\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output,label)\n",
        "        # update running validation loss \n",
        "        valid_loss += loss.item() * data.size(0) # bug\n",
        "    \n",
        "    # print training/validation statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss / len(train_loader.sampler)\n",
        "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
        "    \n",
        "    end = time.time()\n",
        "    print('Vanilla: Epoch:{} Elapsed time:{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        timer(start, end),\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "        ))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h1Udgu4iOpw"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUmSUnrUiOoK"
      },
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "model.eval() # prep model for evaluation\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(len(target)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.sampler)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPHJYC0qiObu"
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS-j8OjqUR75"
      },
      "source": [
        "images, labels = dataiter.next()\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds = torch.max(output, 1)\n",
        "# prep images for display\n",
        "images = images.numpy()\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFLBewhmUYwc"
      },
      "source": [
        "fail = False\n",
        "for images, labels in dataiter:\n",
        "    # get sample outputs\n",
        "    output = model(images)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, preds = torch.max(output, 1)\n",
        "    # prep images for display\n",
        "    images = images.numpy()\n",
        "    # plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(25, 4))\n",
        "    for idx in np.arange(20):\n",
        "        ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "        ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "        ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
        "        if preds[idx] != labels[idx]:\n",
        "            fail = True\n",
        "    plt.show()\n",
        "    if fail:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-CPbmYGJgsg"
      },
      "source": [
        "모델의 학습 파라미터 수 비교"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz5uSGkbiLUe"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCe2VYUtUi1d"
      },
      "source": [
        "count_parameters(model_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxo8zMjCUk0D"
      },
      "source": [
        "count_parameters(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV02OlgClSB_"
      },
      "source": [
        "count_parameters(model_mlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_tH3IoNUnDP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}